#
# Condor submit script to run PBLS on the OSG.  The job, "exec_pipeline.sh", is
# executed in units of chunks of one star's full periodogram.  The instruction
# for the specific subset is transfered as an argument, and the pre-packaged
# light curve as a tarball.  The environment on which the pipeline is run is
# transferred as its own tarball as well.  Output is returned as tarballs to
# $DATA/pbls_results.
#
# Usage:
#  ```[bash]
#  cd ../
#  ./build_the_container_image.sh
#  python clean_directories.py
#  condor_submit condor_submit.sub
#  ```
#
universe    = vanilla

+SingularityImage = "osdf:///ospool/ap21/data/ekul/python_311_3f829b.sif"

executable  = exec_pipeline.sh
arguments = $(star_id) $(period_chunk_ix) $(N_total_chunks)

transfer_input_files = run_pbls_chunk.py, /ospool/ap21/data/ekul/Kepler/$(star_id).tar.gz

should_transfer_files   = Yes
when_to_transfer_output = ON_EXIT
transfer_output_remaps  = "joboutput_$(star_id)_$(period_chunk_ix)_$(N_total_chunks).tar.gz=/ospool/ap21/data/ekul/pbls_results/$(star_id)/joboutput_$(star_id)_$(period_chunk_ix)_$(N_total_chunks).tar.gz"

log           = logs/$(star_id)/job_$(star_id)_$(period_chunk_ix)_$(Cluster)_$(Process).log
error         = logs/$(star_id)/job_$(star_id)_$(period_chunk_ix)_$(Cluster)_$(Process).err
output        = logs/$(star_id)/job_$(star_id)_$(period_chunk_ix)_$(Cluster)_$(Process).out

+JobDurationCategory = "Medium"

request_cpus    = 1 
request_memory  = 2GB
request_disk    = 10GB

queue star_id,period_chunk_ix,N_total_chunks from debug_jobs.joblist
